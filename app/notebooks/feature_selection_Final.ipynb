{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Feature-Selection (97 % gain) & Re-evaluation – după split CT / NT\n",
    "---------------------------------------------------------------------\n",
    "Rulează:\n",
    "    python feature_gain_reanalysis.py\n",
    "\n",
    "Pentru fiecare sistem (CT, NT):\n",
    "1. Încarcă dataset-ul procesat dedicat (Processed_Database_CT / NT).\n",
    "2. Încarcă modelul XGBoost salvat.\n",
    "3. Calculează importanțele (gain) și păstrează coloanele cu 90 % gain cumulativ.\n",
    "4. Re-antrenează modelul pe subsetul redus şi compară performanţele (R², RMSE, MAE).\n"
   ],
   "id": "2b52c4783af05fd3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T19:35:26.612473Z",
     "start_time": "2025-05-20T19:34:24.990897Z"
    }
   },
   "source": [
    "\"\"\"Train & tune XGBoost on 97 %‑gain subset (CT & NT) + metric summary\n",
    "------------------------------------------------------------------------\n",
    "Rulează:\n",
    "    python train_reduced_xgb_cv.py\n",
    "\n",
    "Output‑uri cheie:\n",
    "• modele salvate în models/…_reduced97.pkl\n",
    "• fișier JSON cu feature‑uri păstrate outputs/reduced_feature_lists.json\n",
    "• tabel rezumat cu metricile FULL vs REDUCED + dif. procentuale.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import json, math, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "GAIN_THRESHOLD = 0.97\n",
    "OUT_MODELS = Path(\"models\");\n",
    "OUT_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "OUT_FEATURES = Path(\"outputs/reduced_feature_lists.json\");\n",
    "OUT_FEATURES.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CFG = [\n",
    "    {\n",
    "        \"label\": \"Yield of CT\",\n",
    "        \"data_path\": Path(\"datasets/Processed_Database_CT.csv\"),\n",
    "        \"target\": \"Yield of CT\",\n",
    "        \"model_full\": Path(\"best_model_Yield_of_CT_XGBoost.pkl\"),\n",
    "        \"model_out\": OUT_MODELS / \"best_model_Yield_of_CT_XGBoost97.pkl\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Yield of NT\",\n",
    "        \"data_path\": Path(\"datasets/Processed_Database_NT.csv\"),\n",
    "        \"target\": \"Yield of NT\",\n",
    "        \"model_full\": Path(\"best_model_Yield_of_NT_XGBoost.pkl\"),\n",
    "        \"model_out\": OUT_MODELS / \"best_model_Yield_of_NT_XGBoost97.pkl\",\n",
    "    },\n",
    "]\n",
    "\n",
    "PARAM_GRID = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 1.0],\n",
    "    \"gamma\": [0, 1],\n",
    "}\n",
    "\n",
    "\n",
    "# -------------- Helpers --------------\n",
    "\n",
    "def get_gain_importance(model: xgb.XGBRegressor) -> pd.DataFrame:\n",
    "    imp = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    df = pd.DataFrame(imp.items(), columns=[\"feature\", \"gain\"]).sort_values(\"gain\", ascending=False)\n",
    "    df[\"cum_gain\"] = df[\"gain\"].cumsum() / df[\"gain\"].sum()\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def evaluate(model: xgb.XGBRegressor, X, y):\n",
    "    pred = model.predict(X)\n",
    "    return {\n",
    "        \"R2\": r2_score(y, pred),\n",
    "        \"RMSE\": mean_squared_error(y, pred, squared=False),\n",
    "        \"MAE\": mean_absolute_error(y, pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------- Main loop --------------\n",
    "feature_dict: dict[str, list[str]] = {}\n",
    "results = []\n",
    "\n",
    "for cfg in DATA_CFG:\n",
    "    label = cfg[\"label\"]\n",
    "    print(f\"\\n===== {label} =====\")\n",
    "\n",
    "    # 1. Data split\n",
    "    df = pd.read_csv(cfg[\"data_path\"])\n",
    "    X = df.drop(columns=[cfg[\"target\"]]);\n",
    "    y = df[cfg[\"target\"]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED)\n",
    "\n",
    "    # 2. Feature subset via gain from full model\n",
    "    with open(cfg[\"model_full\"], \"rb\") as f:\n",
    "        model_full: xgb.XGBRegressor = pickle.load(f)\n",
    "    imp_df = get_gain_importance(model_full)\n",
    "    sel_feats = imp_df.loc[imp_df[\"cum_gain\"] <= GAIN_THRESHOLD, \"feature\"].tolist()\n",
    "    feature_dict[label] = sel_feats\n",
    "    print(f\"Păstrăm {len(sel_feats)} / {X.shape[1]} feature‑uri (≥ {GAIN_THRESHOLD * 100:.0f}% gain)\")\n",
    "    X_train_sel, X_test_sel = X_train[sel_feats], X_test[sel_feats]\n",
    "\n",
    "    # 3. Hyper‑parameter tuning pe subset\n",
    "    base = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=SEED, n_jobs=-1)\n",
    "    grid = GridSearchCV(base, PARAM_GRID, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, verbose=0)\n",
    "    grid.fit(X_train_sel, y_train)\n",
    "    best_model: xgb.XGBRegressor = grid.best_estimator_\n",
    "    print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "    # 4. Metrics\n",
    "    m_full = evaluate(model_full, X_test, y_test)\n",
    "    m_red = evaluate(best_model, X_test_sel, y_test)\n",
    "    print(\"Metrics FULL   —  R²={R2:.3f}  RMSE={RMSE:.1f}  MAE={MAE:.1f}\".format(**m_full))\n",
    "    print(\"Metrics REDUCED —  R²={R2:.3f}  RMSE={RMSE:.1f}  MAE={MAE:.1f}\".format(**m_red))\n",
    "    results.append({\"label\": label, \"set\": \"Full\", **m_full})\n",
    "    results.append({\"label\": label, \"set\": \"Reduced\", **m_red})\n",
    "\n",
    "    # 5. Save tuned model\n",
    "    with open(cfg[\"model_out\"], \"wb\") as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    print(\"Model salvat la:\", cfg[\"model_out\"].resolve())\n",
    "\n",
    "# 6. Save feature lists\n",
    "with open(OUT_FEATURES, \"w\") as fp:\n",
    "    json.dump(feature_dict, fp, indent=2)\n",
    "print(\"\\nListele de feature‑uri salvate în:\", OUT_FEATURES.resolve())\n",
    "\n",
    "# 7. Summary table\n",
    "res_df = pd.DataFrame(results)\n",
    "print(\"\\n===== Rezumat =====\")\n",
    "print(res_df.pivot(index=\"label\", columns=\"set\"))\n",
    "\n",
    "print(\"\\nDiferențe procentuale (Reduced vs Full):\")\n",
    "for lbl in res_df[\"label\"].unique():\n",
    "    full = res_df[(res_df.label == lbl) & (res_df.set == \"Full\")].iloc[0]\n",
    "    red = res_df[(res_df.label == lbl) & (res_df.set == \"Reduced\")].iloc[0]\n",
    "    pct = lambda a, b: (a - b) / b * 100 if b != 0 else 0\n",
    "    print(\n",
    "        f\"{lbl}: ΔR²={pct(red.R2, full.R2):+.2f}%   ΔRMSE={pct(red.RMSE, full.RMSE):+.2f}%   ΔMAE={pct(red.MAE, full.MAE):+.2f}%\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Yield of CT =====\n",
      "Păstrăm 24 / 32 feature‑uri (≥ 97% gain)\n",
      "Best params: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Metrics FULL   —  R²=0.820  RMSE=1576.9  MAE=1064.0\n",
      "Metrics REDUCED —  R²=0.822  RMSE=1568.2  MAE=1052.8\n",
      "Model salvat la: /Users/robertochiper/PycharmProjects/SmartAgriculture_iteratia1/models/best_model_Yield_of_CT_XGBoost97.pkl\n",
      "\n",
      "===== Yield of NT =====\n",
      "Păstrăm 24 / 32 feature‑uri (≥ 97% gain)\n",
      "Best params: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Metrics FULL   —  R²=0.763  RMSE=1687.9  MAE=1102.7\n",
      "Metrics REDUCED —  R²=0.766  RMSE=1679.1  MAE=1094.0\n",
      "Model salvat la: /Users/robertochiper/PycharmProjects/SmartAgriculture_iteratia1/models/best_model_Yield_of_NT_XGBoost97.pkl\n",
      "\n",
      "Listele de feature‑uri salvate în: /Users/robertochiper/PycharmProjects/SmartAgriculture_iteratia1/outputs/reduced_feature_lists.json\n",
      "\n",
      "===== Rezumat =====\n",
      "                   R2                   RMSE                       MAE  \\\n",
      "set              Full   Reduced         Full      Reduced         Full   \n",
      "label                                                                    \n",
      "Yield of CT  0.820054  0.822037  1576.913313  1568.203418  1064.012670   \n",
      "Yield of NT  0.763412  0.765878  1687.922532  1679.100281  1102.700067   \n",
      "\n",
      "                         \n",
      "set             Reduced  \n",
      "label                    \n",
      "Yield of CT  1052.78926  \n",
      "Yield of NT  1094.00456  \n",
      "\n",
      "Diferențe procentuale (Reduced vs Full):\n",
      "Yield of CT: ΔR²=+0.24%   ΔRMSE=-0.55%   ΔMAE=-1.05%\n",
      "Yield of NT: ΔR²=+0.32%   ΔRMSE=-0.52%   ΔMAE=-0.79%\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
